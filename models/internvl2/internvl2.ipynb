{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/internvl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlashAttention is not installed.\n",
      "Warning: Flash Attention is not available, use_flash_attn is set to False.\n",
      "Warning: Flash Attention is not available, use_flash_attn is set to False.\n",
      "Warning: Flash Attention is not available, use_flash_attn is set to False.\n",
      "Warning: Flash Attention is not available, use_flash_attn is set to False.\n",
      "Warning: Flash Attention is not available, use_flash_attn is set to False.\n",
      "Warning: Flash Attention is not available, use_flash_attn is set to False.\n",
      "Warning: Flash Attention is not available, use_flash_attn is set to False.\n",
      "Warning: Flash Attention is not available, use_flash_attn is set to False.\n",
      "Warning: Flash Attention is not available, use_flash_attn is set to False.\n",
      "Warning: Flash Attention is not available, use_flash_attn is set to False.\n",
      "Warning: Flash Attention is not available, use_flash_attn is set to False.\n",
      "Warning: Flash Attention is not available, use_flash_attn is set to False.\n",
      "Warning: Flash Attention is not available, use_flash_attn is set to False.\n",
      "Warning: Flash Attention is not available, use_flash_attn is set to False.\n",
      "Warning: Flash Attention is not available, use_flash_attn is set to False.\n",
      "Warning: Flash Attention is not available, use_flash_attn is set to False.\n",
      "Warning: Flash Attention is not available, use_flash_attn is set to False.\n",
      "Warning: Flash Attention is not available, use_flash_attn is set to False.\n",
      "Warning: Flash Attention is not available, use_flash_attn is set to False.\n",
      "Warning: Flash Attention is not available, use_flash_attn is set to False.\n",
      "Warning: Flash Attention is not available, use_flash_attn is set to False.\n",
      "Warning: Flash Attention is not available, use_flash_attn is set to False.\n",
      "Warning: Flash Attention is not available, use_flash_attn is set to False.\n",
      "Warning: Flash Attention is not available, use_flash_attn is set to False.\n",
      "Warning: Flash attention is not available, using eager attention instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/internvl/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  5.73it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from decord import VideoReader, cpu\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "\n",
    "def build_transform(input_size):\n",
    "    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=MEAN, std=STD)\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    best_ratio_diff = float('inf')\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=6, image_size=448, use_thumbnail=False):\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # calculate the existing image aspect ratio\n",
    "    target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "        i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    # find the closest aspect ratio to the target\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "    # calculate the target width and height\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # resize the image\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size\n",
    "        )\n",
    "        # split the image\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images\n",
    "\n",
    "\n",
    "def load_image(image_file, input_size=448, max_num=6):\n",
    "    image = Image.open(image_file).convert('RGB')\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "    pixel_values = [transform(image) for image in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return pixel_values\n",
    "\n",
    "\n",
    "path = 'InternVL2-8B'\n",
    "model = AutoModel.from_pretrained(\n",
    "    path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,\n",
    "    ).eval().cuda()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the max number of tiles in `max_num`\n",
    "pixel_values = load_image('./InternVL2-8B/examples/image1.jpg', max_num=6).to(torch.bfloat16).cuda()\n",
    "\n",
    "generation_config = dict(\n",
    "    num_beams=1,\n",
    "    max_new_tokens=1024,\n",
    "    do_sample=False,\n",
    ")\n",
    "\n",
    "# pure-text conversation (纯文本对话)\n",
    "question = 'Hello, who are you?'\n",
    "response, history = model.chat(tokenizer, None, question, generation_config, history=None, return_history=True)\n",
    "print(f'User: {question}')\n",
    "print(f'Assistant: {response}')\n",
    "\n",
    "question = 'Can you tell me a story?'\n",
    "response, history = model.chat(tokenizer, None, question, generation_config, history=history, return_history=True)\n",
    "print(f'User: {question}')\n",
    "print(f'Assistant: {response}')\n",
    "\n",
    "# single-image single-round conversation (单图单轮对话)\n",
    "question = '<image>\\nPlease describe the image shortly.'\n",
    "response = model.chat(tokenizer, pixel_values, question, generation_config)\n",
    "print(f'User: {question}')\n",
    "print(f'Assistant: {response}')\n",
    "\n",
    "# single-image multi-round conversation (单图多轮对话)\n",
    "question = '<image>\\nPlease describe the image in detail.'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=None, return_history=True)\n",
    "print(f'User: {question}')\n",
    "print(f'Assistant: {response}')\n",
    "\n",
    "question = 'Please write a poem according to the image.'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=history, return_history=True)\n",
    "print(f'User: {question}')\n",
    "print(f'Assistant: {response}')\n",
    "\n",
    "# multi-image multi-round conversation, combined images (多图多轮对话，拼接图像)\n",
    "pixel_values1 = load_image('./InternVL2-8B/examples/image1.jpg', max_num=6).to(torch.bfloat16).cuda()\n",
    "pixel_values2 = load_image('./InternVL2-8B/examples/image2.jpg', max_num=6).to(torch.bfloat16).cuda()\n",
    "pixel_values = torch.cat((pixel_values1, pixel_values2), dim=0)\n",
    "\n",
    "question = '<image>\\nDescribe the two images in detail.'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config,\n",
    "                               history=None, return_history=True)\n",
    "\n",
    "question = 'What are the similarities and differences between these two images.'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config,\n",
    "                               history=history, return_history=True)\n",
    "print(f'User: {question}')\n",
    "print(f'Assistant: {response}')\n",
    "\n",
    "# multi-image multi-round conversation, separate images (多图多轮对话，独立图像)\n",
    "pixel_values1 = load_image('./InternVL2-8B/examples/image1.jpg', max_num=6).to(torch.bfloat16).cuda()\n",
    "pixel_values2 = load_image('./InternVL2-8B/examples/image2.jpg', max_num=6).to(torch.bfloat16).cuda()\n",
    "pixel_values = torch.cat((pixel_values1, pixel_values2), dim=0)\n",
    "num_patches_list = [pixel_values1.size(0), pixel_values2.size(0)]\n",
    "\n",
    "question = 'Image-1: <image>\\nImage-2: <image>\\nDescribe the two images in detail.'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config,\n",
    "                               num_patches_list=num_patches_list,\n",
    "                               history=None, return_history=True)\n",
    "print(f'User: {question}')\n",
    "print(f'Assistant: {response}')\n",
    "\n",
    "question = 'What are the similarities and differences between these two images.'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config,\n",
    "                               num_patches_list=num_patches_list,\n",
    "                               history=history, return_history=True)\n",
    "print(f'User: {question}')\n",
    "print(f'Assistant: {response}')\n",
    "\n",
    "# batch inference, single image per sample (单图批处理)\n",
    "pixel_values1 = load_image('./examples/image1.jpg', max_num=6).to(torch.bfloat16).cuda()\n",
    "pixel_values2 = load_image('./examples/image2.jpg', max_num=6).to(torch.bfloat16).cuda()\n",
    "num_patches_list = [pixel_values1.size(0), pixel_values2.size(0)]\n",
    "pixel_values = torch.cat((pixel_values1, pixel_values2), dim=0)\n",
    "\n",
    "questions = ['<image>\\nDescribe the image in detail.'] * len(num_patches_list)\n",
    "responses = model.batch_chat(tokenizer, pixel_values,\n",
    "                             num_patches_list=num_patches_list,\n",
    "                             questions=questions,\n",
    "                             generation_config=generation_config)\n",
    "for question, response in zip(questions, responses):\n",
    "    print(f'User: {question}')\n",
    "    print(f'Assistant: {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Let's analyze the image in detail to determine if it meets the specified conditions:\n",
      "\n",
      "1. **Normal Human Proportions and No Body Deformities, Missing Parts, or Distortions:**\n",
      "   - The person in the image has a well-proportioned body. There are no visible deformities, missing parts, or distortions. The proportions of the body appear to be normal.\n",
      "\n",
      "2. **Complete Fingers with No Extra, Missing, or Deformed Fingers:**\n",
      "   - The person in the image has both hands visible. The fingers appear to be complete, with no extra fingers, missing fingers, or deformed fingers. The fingers look normal.\n",
      "\n",
      "3. **Complete Limbs with No Dislocations, Extra Arms, Extra Legs, or Extra Hands:**\n",
      "   - The person in the image has both arms and legs visible. The limbs appear to be complete, with no dislocations, extra arms, extra legs, or extra hands. The limbs look normal.\n",
      "\n",
      "4. **Complete Faces with No Facial Deformities, Missing Parts, or Distortions:**\n",
      "   - The person in the image has a complete face. There are no visible facial deformities, missing parts, or distortions. The face looks normal.\n",
      "\n",
      "In conclusion, the image meets all the specified conditions. The person in the image has normal human proportions, complete fingers, complete limbs, and a complete face without any deformities, missing parts, or distortions.\n"
     ]
    }
   ],
   "source": [
    "pixel_values = load_image('images/bad3.png', max_num=6).to(torch.bfloat16).cuda()\n",
    "\n",
    "generation_config = dict(\n",
    "    num_beams=1,\n",
    "    max_new_tokens = 1024,\n",
    "    do_sample=False,\n",
    ")\n",
    "# question = '<image>\\nYou are given an image and your task is to analyze it based on the following criteria.your ouput format of json is \"{\"human_portrait\": {\"exists\": true/false,\"face_integrity\": {\"complete\": true/false,\"face_distortion,missing_parts or 怪鱼\":true/false}, \"finger_integrity\": {\"complete\": true/false,\"foot or hand extra_fingers,missing_fingers or deformed_fingers\":true/false},\"limb_integrity\": {\"complete\": true/false,\"dislocated_limbs,incorrect_connections,extra_arms,extra_legs,extra_hands\":true/false}},\"inappropriate_content\": {\"exists\": true/false,\"details\": [\"adult_content\", \"graphic_violence\", \"disturbing_content\"]}}\"'\n",
    "question = '<image>You are a discerning inspection engineer with the ability to identify defects in images. Please help me determine if the image exhibits any of the following conditions:1.The people in the image should have normal human proportions and should not display any body deformities, missing parts, or distortions.2.If there exist fingers ,their fingers should be complete, with no extra fingers, missing fingers, or deformed fingers.3.Their limbs should be complete, with no dislocations, incorrect connections, extra arms, extra legs, or extra hands.4.If there exist faces, their faces should be complete, with no facial deformities, missing parts, or distortions.'\n",
    "response = model.chat(tokenizer, pixel_values, question, generation_config)\n",
    "\n",
    "print(f'Assistant: {response}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num = 6\n",
    "min_num = 1\n",
    "target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "        i * j <= max_num and i * j >= min_num)\n",
    "print(target_ratios)\n",
    "target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "print(target_ratios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "internvl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
